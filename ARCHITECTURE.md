# ðŸ—ï¸ System Architecture

## Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FastAPI Application                      â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  REST API  â”‚  â”‚  Scheduler   â”‚  â”‚ Scraper Manager  â”‚   â”‚
â”‚  â”‚ Endpoints  â”‚  â”‚ (APScheduler)â”‚  â”‚                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚               â”‚                    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚               â”‚                    â”‚
          â”‚               â”‚                    â”‚
          â–¼               â–¼                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Database â”‚    â”‚  Cron    â”‚      â”‚   Scrapers   â”‚
    â”‚ (SQLite/ â”‚    â”‚  Jobs    â”‚      â”‚              â”‚
    â”‚  Postgres)â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚ â”‚Culture.govâ”‚ â”‚
                                       â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
                                       â”‚ â”‚VisitGreeceâ”‚ â”‚
                                       â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
                                       â”‚ â”‚Pigolampidesâ”‚ â”‚
                                       â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
                                       â”‚ â”‚ More.com â”‚ â”‚
                                       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Flow

### 1. Application Startup

```
Start API
    â”‚
    â”œâ”€> Initialize Database (create tables)
    â”‚
    â”œâ”€> Start Scheduler
    â”‚       â”‚
    â”‚       â””â”€> Load schedule from env (SCRAPER_SCHEDULE)
    â”‚       â””â”€> Register scraping jobs
    â”‚       â””â”€> Run initial scrape (if SCRAPER_RUN_ON_STARTUP=True)
    â”‚
    â””â”€> Start FastAPI server
```

### 2. Scheduled Scraping Flow

```
Scheduler Trigger (e.g., every 6 hours)
    â”‚
    â”œâ”€> Create Database Session
    â”‚
    â”œâ”€> Initialize Scraper Manager
    â”‚
    â”œâ”€> Run All Scrapers Sequentially:
    â”‚       â”‚
    â”‚       â”œâ”€> Culture.gov Scraper
    â”‚       â”‚       â”œâ”€> Navigate to website
    â”‚       â”‚       â”œâ”€> Load all events
    â”‚       â”‚       â”œâ”€> Extract data
    â”‚       â”‚       â””â”€> Return events list
    â”‚       â”‚
    â”‚       â”œâ”€> VisitGreece Scraper
    â”‚       â”‚       â””â”€> (same process)
    â”‚       â”‚
    â”‚       â”œâ”€> Pigolampides Scraper
    â”‚       â”‚       â””â”€> (same process)
    â”‚       â”‚
    â”‚       â””â”€> More.com Scraper
    â”‚               â””â”€> (same process)
    â”‚
    â”œâ”€> Save Events to Database
    â”‚       â”œâ”€> Check for duplicates (by URL)
    â”‚       â”œâ”€> Insert new events
    â”‚       â””â”€> Commit transaction
    â”‚
    â””â”€> Log Results
```

### 3. API Request Flow

```
Client Request: GET /events?source=culture_gov&limit=20
    â”‚
    â”œâ”€> FastAPI receives request
    â”‚
    â”œâ”€> Validate query parameters
    â”‚
    â”œâ”€> Create database session
    â”‚
    â”œâ”€> Query database:
    â”‚       SELECT * FROM events
    â”‚       WHERE source = 'culture_gov'
    â”‚       ORDER BY created_at DESC
    â”‚       LIMIT 20
    â”‚
    â”œâ”€> Serialize to JSON (Pydantic models)
    â”‚
    â””â”€> Return response to client
```

### 4. Manual Scrape Trigger

```
Client Request: POST /scrape?headless=true&max_events=100
    â”‚
    â”œâ”€> FastAPI receives request
    â”‚
    â”œâ”€> Add scraping task to background queue
    â”‚
    â”œâ”€> Return immediate response: "Scraping started"
    â”‚
    â””â”€> Background Task:
            â”œâ”€> Create database session
            â”œâ”€> Run Scraper Manager
            â”œâ”€> Save results to database
            â””â”€> Log completion
```

## Data Flow

### Scraping Process

```
Website â†’ Selenium WebDriver â†’ Scraper Class â†’ Raw Data
                                                    â”‚
                                                    â–¼
                                            Data Extraction
                                            (title, date, etc.)
                                                    â”‚
                                                    â–¼
                                            Scraper Manager
                                                    â”‚
                                                    â–¼
                                            Database Models
                                            (Event/Deal objects)
                                                    â”‚
                                                    â–¼
                                            SQLAlchemy ORM
                                                    â”‚
                                                    â–¼
                                            Database Storage
```

### API Response Process

```
Database â†’ SQLAlchemy Query â†’ Python Objects â†’ Pydantic Models â†’ JSON â†’ Client
```

## Database Schema

### Events Table

```sql
CREATE TABLE events (
    id INTEGER PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    description TEXT,
    date VARCHAR(100),
    location VARCHAR(300),
    category VARCHAR(100),
    price VARCHAR(100),
    url VARCHAR(500) UNIQUE,
    source VARCHAR(100) NOT NULL,
    images JSON,
    contact VARCHAR(300),
    content JSON,
    full_text TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_events_source ON events(source);
CREATE INDEX idx_events_category ON events(category);
CREATE INDEX idx_events_url ON events(url);
```

### Deals Table

```sql
CREATE TABLE deals (
    id INTEGER PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    description TEXT,
    price VARCHAR(100),
    original_price VARCHAR(100),
    discount VARCHAR(50),
    url VARCHAR(500) UNIQUE,
    source VARCHAR(100) NOT NULL,
    images JSON,
    category VARCHAR(100),
    valid_until VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_deals_source ON deals(source);
CREATE INDEX idx_deals_category ON deals(category);
CREATE INDEX idx_deals_url ON deals(url);
```

## Scheduler Configuration

### Schedule Options

| Option | Cron Expression | Description |
|--------|----------------|-------------|
| `hourly` | `0 * * * *` | Every hour at :00 |
| `every_6_hours` | `0 */6 * * *` | Every 6 hours |
| `every_12_hours` | `0 */12 * * *` | Every 12 hours |
| `twice_daily` | `0 6,18 * * *` | 6 AM and 6 PM |
| `daily` | `0 2 * * *` | Daily at 2 AM |

### Scheduler Lifecycle

```
Application Start
    â”‚
    â”œâ”€> Create BackgroundScheduler instance
    â”‚
    â”œâ”€> Add job with CronTrigger
    â”‚       â”œâ”€> Job ID: scraper_6h
    â”‚       â”œâ”€> Function: scrape_job()
    â”‚       â””â”€> Schedule: every 6 hours
    â”‚
    â”œâ”€> Start scheduler
    â”‚
    â””â”€> Scheduler runs in background thread
            â”‚
            â”œâ”€> Wait for trigger time
            â”‚
            â”œâ”€> Execute scrape_job()
            â”‚
            â”œâ”€> Calculate next run time
            â”‚
            â””â”€> Repeat
```

## Deployment Architecture

### Single Instance (Small Scale)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Cloud Platform              â”‚
â”‚  (Railway/Render/DigitalOcean)      â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Docker Container            â”‚ â”‚
â”‚  â”‚                               â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚   API   â”‚  â”‚ Scheduler  â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                               â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚      Scrapers           â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   PostgreSQL Database         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multi-Instance (Large Scale)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Load Balancer                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                      â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  API Instance  â”‚    â”‚  API Instance  â”‚
       â”‚   (Read Only)  â”‚    â”‚   (Read Only)  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚      Worker Instance               â”‚
       â”‚  (Scraping + Scheduler Only)       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   PostgreSQL (Primary)             â”‚
       â”‚                                    â”‚
       â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
       â”‚   â”‚  Read Replicas (Optional)â”‚    â”‚
       â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Security Considerations

### API Security

```
Client Request
    â”‚
    â”œâ”€> CORS Middleware (validate origin)
    â”‚
    â”œâ”€> Rate Limiting (optional)
    â”‚
    â”œâ”€> Authentication (optional)
    â”‚
    â”œâ”€> Input Validation (Pydantic)
    â”‚
    â””â”€> Process Request
```

### Scraping Security

- Use headless mode in production
- Rotate user agents
- Respect robots.txt
- Implement rate limiting
- Handle errors gracefully

## Monitoring & Logging

### Key Metrics to Monitor

1. **Scheduler Status**
   - Is scheduler running?
   - Next run time
   - Last run success/failure

2. **Scraping Metrics**
   - Events scraped per source
   - Success/failure rate
   - Scraping duration

3. **API Metrics**
   - Request count
   - Response time
   - Error rate

4. **Database Metrics**
   - Total events/deals
   - Growth rate
   - Query performance

### Logging Flow

```
Application â†’ Python Logger â†’ Console/File
                                    â”‚
                                    â–¼
                            Cloud Platform Logs
                            (Railway/Render/etc.)
```

## Performance Optimization

### Database Optimization

- Index on frequently queried columns (source, category, url)
- Use connection pooling
- Implement caching (Redis) for frequent queries
- Use read replicas for scaling

### Scraping Optimization

- Run scrapers in parallel (with caution)
- Implement resume capability
- Cache scraped data temporarily
- Use headless mode to save resources

### API Optimization

- Implement pagination (skip/limit)
- Add response caching
- Use async endpoints for long operations
- Compress responses (gzip)

## Error Handling

### Scraper Errors

```
Scraper Error
    â”‚
    â”œâ”€> Log error with traceback
    â”‚
    â”œâ”€> Continue with next scraper
    â”‚
    â””â”€> Return partial results
```

### API Errors

```
API Error
    â”‚
    â”œâ”€> Catch exception
    â”‚
    â”œâ”€> Log error
    â”‚
    â”œâ”€> Return appropriate HTTP status
    â”‚       â”œâ”€> 400: Bad Request
    â”‚       â”œâ”€> 404: Not Found
    â”‚       â”œâ”€> 500: Internal Server Error
    â”‚
    â””â”€> Include error message in response
```

## Scaling Strategy

### Vertical Scaling
- Increase instance size
- More CPU/RAM for scrapers
- Faster database

### Horizontal Scaling
- Multiple API instances (read-only)
- Dedicated worker for scraping
- Database read replicas
- Load balancer

### Optimization
- Cache frequently accessed data
- Optimize database queries
- Reduce scraping frequency
- Implement incremental scraping
