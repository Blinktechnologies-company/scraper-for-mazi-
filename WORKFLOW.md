# Complete Workflow Guide

## Overview

This document explains the complete workflow from scraping to API delivery.

---

## ğŸ”„ Automatic Workflow (Scheduled)

### When Scheduler Triggers

```
1. Scheduler Trigger (e.g., every 6 hours)
        â†“
2. Scraper Manager Starts
        â†“
3. Run All Scrapers Sequentially
        â”œâ”€â†’ Culture.gov Scraper
        â”‚       â”œâ”€ Navigate to website
        â”‚       â”œâ”€ Load all events
        â”‚       â”œâ”€ Extract data
        â”‚       â””â”€ Return raw events
        â”‚
        â”œâ”€â†’ VisitGreece Scraper
        â”‚       â””â”€ (same process)
        â”‚
        â”œâ”€â†’ Pigolampides Scraper
        â”‚       â””â”€ (same process)
        â”‚
        â””â”€â†’ More.com Scraper
                â””â”€ (same process)
        â†“
4. Collect Raw Events by Source
   {
     'culture_gov': [event1, event2, ...],
     'visitgreece': [event1, event2, ...],
     'pigolampides': [event1, event2, ...],
     'more_events': [event1, event2, ...]
   }
        â†“
5. Data Transformer
        â”œâ”€ Clean text
        â”œâ”€ Format dates â†’ YYYY-MM-DD
        â”œâ”€ Map categories â†’ Standard names
        â”œâ”€ Detect regions â†’ Greek regions
        â”œâ”€ Extract images â†’ First image URL
        â”œâ”€ Convert prices â†’ Integer (0 for free)
        â””â”€ Assign IDs â†’ Sequential
        â†“
6. Standardized Events Array
   [
     {id: 1, title: "...", date: "2026-02-09", ...},
     {id: 2, title: "...", date: "2026-02-10", ...},
     ...
   ]
        â†“
7. Save to Two Locations
        â”œâ”€â†’ Combined JSON File
        â”‚   scraped_data/combined_events.json
        â”‚
        â””â”€â†’ Database (PostgreSQL/SQLite)
            â”œâ”€ Check for duplicates (by URL)
            â”œâ”€ Insert new events
            â””â”€ Commit transaction
        â†“
8. Log Results
   âœ“ Total events: 450
   âœ“ By source: {culture_gov: 120, visitgreece: 180, ...}
   âœ“ Combined JSON: scraped_data/combined_events.json
        â†“
9. Wait for Next Trigger
```

---

## ğŸ¯ Manual Workflow (API Trigger)

### When User Calls POST /scrape

```
1. API Receives Request
   POST /scrape?headless=true&max_events=100
        â†“
2. Add to Background Queue
        â†“
3. Return Immediate Response
   {"status": "started", "message": "Scraping started in background"}
        â†“
4. Background Task Runs
   (Same as automatic workflow steps 2-8)
        â†“
5. User Can Check Progress
   GET /scheduler/status
   GET /stats
```

---

## ğŸ“Š Data Transformation Details

### Example: Culture.gov Event

**Raw Data (from scraper):**
```json
{
  "title": "Ancient Greek Theater",
  "content": [
    "A magnificent performance",
    "Featuring renowned actors"
  ],
  "date": "15/02/2026",
  "location": "Odeon of Herodes Atticus, Athens",
  "url": "https://culture.gov.gr/event1",
  "images": [
    "https://culture.gov.gr/image1.jpg",
    "https://culture.gov.gr/image2.jpg"
  ],
  "full_text": "Long text content..."
}
```

**Transformed Data:**
```json
{
  "id": 1,
  "title": "Ancient Greek Theater",
  "description": "A magnificent performance Featuring renowned actors",
  "date": "2026-02-15",
  "schedule": null,
  "region": "Î‘Ï„Ï„Î¹ÎºÎ®",
  "category": "Theater",
  "categoryColor": "#9B59B6",
  "subCategories": null,
  "location": "Odeon of Herodes Atticus, Athens",
  "venue": "Odeon of Herodes Atticus, Athens",
  "venueUrl": null,
  "url": "https://culture.gov.gr/event1",
  "eventUrl": "https://culture.gov.gr/event1",
  "image": "https://culture.gov.gr/image1.jpg",
  "imageUrl": "https://culture.gov.gr/image1.jpg",
  "price": 0,
  "maxCapacity": 100,
  "targetAges": null,
  "specialFeatures": null,
  "source": "Culture.gov.gr"
}
```

**Transformations Applied:**
1. âœ… Description: Joined content array
2. âœ… Date: `15/02/2026` â†’ `2026-02-15`
3. âœ… Region: Detected "Athens" â†’ "Î‘Ï„Ï„Î¹ÎºÎ®"
4. âœ… Category: Detected "Theater" from title
5. âœ… Category Color: Mapped to `#9B59B6`
6. âœ… Image: Extracted first image
7. âœ… Price: No price found â†’ `0`
8. âœ… Source: Formatted to "Culture.gov.gr"

---

## ğŸŒ API Access Workflow

### Client Requests Events

```
1. Client Makes Request
   GET /events?category=Theater&limit=20
        â†“
2. API Receives Request
        â†“
3. Validate Parameters
   âœ“ category: valid
   âœ“ limit: within range (1-200)
        â†“
4. Query Database
   SELECT * FROM events
   WHERE category = 'Theater'
   ORDER BY created_at DESC
   LIMIT 20
        â†“
5. Serialize Results (Pydantic)
   Convert DB objects â†’ JSON
        â†“
6. Return Response
   [
     {id: 1, title: "...", ...},
     {id: 2, title: "...", ...},
     ...
   ]
```

### Client Requests Combined JSON

```
1. Client Makes Request
   GET /combined-events
        â†“
2. API Receives Request
        â†“
3. Read JSON File
   scraped_data/combined_events.json
        â†“
4. Return File Contents
   [all events in standardized format]
```

---

## ğŸ” Search & Filter Workflow

### Example: Search for "concert" in Athens

```
1. Request
   GET /events?search=concert&region=Î‘Ï„Ï„Î¹ÎºÎ®
        â†“
2. Build Query
   SELECT * FROM events
   WHERE (title LIKE '%concert%' OR description LIKE '%concert%')
   AND content->>'region' = 'Î‘Ï„Ï„Î¹ÎºÎ®'
   ORDER BY created_at DESC
        â†“
3. Execute Query
        â†“
4. Return Matching Events
```

---

## ğŸ“… Scheduler Workflow

### Initialization

```
1. API Starts
        â†“
2. Initialize Database
   Create tables if not exist
        â†“
3. Start Scheduler
        â”œâ”€ Load schedule from env (SCRAPER_SCHEDULE)
        â”œâ”€ Create cron trigger
        â””â”€ Register scraping job
        â†“
4. Scheduler Running
   âœ“ Next run: 2026-01-19 18:00:00
```

### Job Execution

```
1. Cron Trigger Fires
   (e.g., every 6 hours at :00)
        â†“
2. Execute Scraping Job
   (See Automatic Workflow above)
        â†“
3. Calculate Next Run Time
   Current: 12:00
   Next: 18:00 (6 hours later)
        â†“
4. Wait for Next Trigger
```

---

## ğŸ³ Docker Workflow

### Startup

```
1. docker-compose up
        â†“
2. Start PostgreSQL Container
   âœ“ Database ready
        â†“
3. Start API Container
        â”œâ”€ Install dependencies
        â”œâ”€ Initialize database
        â”œâ”€ Start scheduler
        â””â”€ Start FastAPI server
        â†“
4. Services Running
   âœ“ API: http://localhost:8000
   âœ“ Database: localhost:5432
   âœ“ Scheduler: Active
```

---

## â˜ï¸ Cloud Deployment Workflow

### Railway Example

```
1. Push Code to GitHub
        â†“
2. Railway Detects Changes
        â†“
3. Build Docker Image
   â”œâ”€ Install Chrome
   â”œâ”€ Install Python dependencies
   â””â”€ Copy application code
        â†“
4. Deploy Container
   â”œâ”€ Set environment variables
   â”œâ”€ Connect to PostgreSQL
   â””â”€ Start application
        â†“
5. Application Running
   âœ“ API: https://your-app.railway.app
   âœ“ Scheduler: Active
   âœ“ Database: Connected
        â†“
6. Continuous Operation
   â”œâ”€ Scheduler runs on schedule
   â”œâ”€ API serves requests
   â””â”€ Auto-restart on failure
```

---

## ğŸ”„ Complete Lifecycle

### Day 1: Initial Setup

```
1. Deploy to Railway
2. Scheduler starts
3. Initial scrape runs (if SCRAPER_RUN_ON_STARTUP=True)
4. ~400 events collected
5. Combined JSON created
6. Database populated
7. API ready for requests
```

### Day 2-âˆ: Continuous Operation

```
Every 6 hours:
  â”œâ”€ Scheduler triggers
  â”œâ”€ Scrapers run
  â”œâ”€ New events collected
  â”œâ”€ Data transformed
  â”œâ”€ Combined JSON updated
  â”œâ”€ Database updated (new events only)
  â””â”€ API serves latest data

Anytime:
  â”œâ”€ Clients query API
  â”œâ”€ Get latest events
  â””â”€ Display in frontend
```

---

## ğŸ“Š Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTINUOUS OPERATION                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scheduler   â”‚ â† Every 6 hours
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Scraper Manager                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚Culture.gov â”‚  â”‚VisitGreece â”‚  â”‚Pigolampidesâ”‚  â”‚More.comâ”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”˜
         â”‚               â”‚               â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Raw Events Dict  â”‚
                    â”‚  by Source       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Data Transformer â”‚
                    â”‚  - Clean         â”‚
                    â”‚  - Format        â”‚
                    â”‚  - Standardize   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Standardized     â”‚
                    â”‚ Events Array     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                   â”‚
                    â–¼                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Combined JSON   â”‚  â”‚  Database    â”‚
          â”‚  File            â”‚  â”‚  (Postgres)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   FastAPI        â”‚
                    â”‚   Endpoints      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Your Frontend  â”‚
                    â”‚   / Clients      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Key Points

1. **Automatic**: Runs on schedule without intervention
2. **Standardized**: All data in consistent format
3. **Dual Storage**: JSON file + Database
4. **API Access**: Multiple endpoints for different needs
5. **Monitoring**: Built-in status and statistics
6. **Scalable**: Easy to add more scrapers or sources
7. **Reliable**: Error handling and retry logic
8. **Cloud Ready**: Deploy anywhere with Docker

---

## ğŸ“ Summary

The system operates in a continuous loop:

1. â° **Scheduler triggers** at configured intervals
2. ğŸ•·ï¸ **Scrapers collect** data from 4 sources
3. ğŸ”€ **Transformer standardizes** all data
4. ğŸ’¾ **Storage saves** to JSON + Database
5. ğŸš€ **API serves** data to clients
6. ğŸ” **Repeat** on next trigger

All while providing:
- Real-time monitoring
- Manual trigger capability
- Flexible querying
- Clean, consistent data

**Your events are always fresh and ready to use!**
