# ‚úÖ Complete Solution - Events & Deals Scraper API

## What You Have Now

A **production-ready, continuously running** web scraping and API system with:

### ‚úÖ Core Features
1. **4 Active Scrapers** - Culture.gov, VisitGreece, Pigolampides, More.com
2. **Continuous Scraping** - Runs automatically on schedule (hourly/daily/etc.)
3. **Data Transformation** - All data standardized into unified format
4. **Combined JSON Export** - Single file with all events in standard format
5. **Database Storage** - PostgreSQL/MySQL/SQLite support
6. **REST API** - FastAPI with full documentation
7. **Docker Ready** - Complete containerization
8. **Cloud Deployment** - Ready for Railway, Render, DigitalOcean, AWS

---

## üéØ Key Innovation: Data Transformation

**Before (Raw Data):**
- Different field names per source
- Inconsistent date formats
- Mixed category names
- Varying data structures

**After (Standardized):**
```json
{
  "id": 1342,
  "title": "Event Title",
  "description": "Clean description",
  "date": "2026-02-09",
  "region": "ŒëœÑœÑŒπŒ∫ŒÆ",
  "category": "Cultural",
  "categoryColor": "#F39C12",
  "location": "Venue address",
  "venue": "Venue name",
  "url": "https://example.com/event",
  "image": "https://example.com/image.jpg",
  "price": 0,
  "source": "More.com"
}
```

---

## üìä Data Flow

```
Scrapers Run (4 sources)
    ‚Üì
Raw Data Collected
    ‚Üì
Data Transformer
    ‚îú‚îÄ Clean text
    ‚îú‚îÄ Format dates (YYYY-MM-DD)
    ‚îú‚îÄ Map categories
    ‚îú‚îÄ Detect regions
    ‚îú‚îÄ Extract images
    ‚îî‚îÄ Convert prices
    ‚Üì
Standardized Events Array
    ‚Üì
    ‚îú‚îÄ‚Üí Combined JSON File (scraped_data/combined_events.json)
    ‚îî‚îÄ‚Üí Database Storage (PostgreSQL/SQLite)
    ‚Üì
REST API Endpoints
    ‚Üì
Your Frontend/App
```

---

## üöÄ How to Deploy

### Option 1: Railway (Recommended - 5 minutes)

```bash
# 1. Install CLI
npm install -g @railway/cli

# 2. Login
railway login

# 3. Initialize
railway init

# 4. Add PostgreSQL
railway add --database postgresql

# 5. Set environment
railway variables set HEADLESS_MODE=True
railway variables set SCRAPER_SCHEDULE=every_6_hours
railway variables set SCRAPER_MAX_EVENTS=100

# 6. Deploy
railway up

# 7. Get URL
railway domain
```

**Done!** Your API is live with continuous scraping.

### Option 2: Docker (Local/VPS)

```bash
# 1. Clone and configure
git clone your-repo
cd your-repo
cp .env.example .env

# 2. Edit .env
nano .env
# Set: SCRAPER_SCHEDULE=every_6_hours

# 3. Run
docker-compose up -d

# 4. Access
http://localhost:8000/docs
```

---

## üì° API Endpoints

### Events
- `GET /events` - All events (paginated, filterable)
- `GET /events/{id}` - Specific event
- `GET /combined-events` - **Combined JSON file** ‚≠ê

### Scraping
- `POST /scrape` - Trigger scraping (background)
- `POST /scrape/sync` - Trigger scraping (wait)

### Monitoring
- `GET /stats` - Statistics
- `GET /scheduler/status` - Scheduler info
- `GET /health` - Health check

### Documentation
- `GET /docs` - Swagger UI
- `GET /redoc` - ReDoc

---

## üîÑ Continuous Scraping Configuration

Set in `.env` or environment variables:

```bash
# Schedule options
SCRAPER_SCHEDULE=hourly           # Every hour
SCRAPER_SCHEDULE=every_6_hours    # Every 6 hours ‚≠ê Recommended
SCRAPER_SCHEDULE=every_12_hours   # Every 12 hours
SCRAPER_SCHEDULE=twice_daily      # 6 AM and 6 PM
SCRAPER_SCHEDULE=daily            # Once daily at 2 AM

# Other settings
SCRAPER_MAX_EVENTS=100            # Max events per source
SCRAPER_RUN_ON_STARTUP=True       # Run immediately on startup
HEADLESS_MODE=True                # Headless browser (production)
```

---

## üìÅ Project Files

### Core Application
- `api.py` - FastAPI server
- `database.py` - Database models
- `scraper_manager.py` - Orchestrates all scrapers
- `scheduler.py` - Background scheduler
- `data_transformer.py` - **Data standardization** ‚≠ê

### Scrapers
- `culture_final_scraper.py`
- `visitgreece_detailed_scraper.py`
- `pigolampides_scraper.py`
- `more_events_scraper_optimized.py`

### Deployment
- `Dockerfile` - Container config
- `docker-compose.yml` - Multi-container setup
- `render.yaml` - Render deployment
- `railway-config.json` - Railway deployment

### Documentation
- `README.md` - Main documentation
- `QUICKSTART.md` - 5-minute deploy
- `DEPLOYMENT.md` - Full deployment guide
- `TRANSFORMER_GUIDE.md` - **Data transformation** ‚≠ê
- `ARCHITECTURE.md` - System architecture
- `PROJECT_SUMMARY.md` - Project overview

### Utilities
- `run_api.py` - Start API
- `run_scrapers.py` - Manual scraping
- `test_api.py` - API tests
- `test_transformer.py` - **Test transformation** ‚≠ê
- `setup.py` - Project setup

---

## üéØ Usage Examples

### 1. Get All Events (Standardized Format)

```bash
curl http://localhost:8000/combined-events
```

Returns:
```json
[
  {
    "id": 1,
    "title": "Event Title",
    "date": "2026-02-09",
    "category": "Cultural",
    "categoryColor": "#F39C12",
    "region": "ŒëœÑœÑŒπŒ∫ŒÆ",
    "source": "More.com",
    ...
  },
  ...
]
```

### 2. Get Events from Database (with filters)

```bash
# By source
curl "http://localhost:8000/events?source=More.com&limit=20"

# By category
curl "http://localhost:8000/events?category=Cultural"

# Search
curl "http://localhost:8000/events?search=concert"
```

### 3. Check Scheduler Status

```bash
curl http://localhost:8000/scheduler/status
```

Returns:
```json
{
  "running": true,
  "jobs": [{
    "id": "scraper_6h",
    "name": "6-Hour Scraper",
    "next_run": "2026-01-19T18:00:00"
  }]
}
```

### 4. Trigger Manual Scraping

```bash
# Background (returns immediately)
curl -X POST "http://localhost:8000/scrape?headless=true&max_events=100"

# Synchronous (waits for completion)
curl -X POST "http://localhost:8000/scrape/sync?headless=true&max_events=50"
```

---

## üìä What Gets Saved

### 1. Combined JSON File
**Location:** `scraped_data/combined_events.json`

All events in standardized format, ready for:
- Frontend consumption
- Data analysis
- Backup/export
- Third-party integration

### 2. Database
**Tables:** `events`, `deals`

Indexed and queryable via API endpoints.

### 3. Individual Source Files (backup)
- `scraped_data/culture_gov_final_events.json`
- `scraped_data/visitgreece_detailed_events.json`
- `scraped_data/pigolampides_blog_posts.json`
- `scraped_data/more_events_optimized.json`

---

## üé® Frontend Integration

### React Example

```javascript
// Fetch all events
const response = await fetch('https://your-api.com/combined-events');
const events = await response.json();

// Display events
events.map(event => (
  <EventCard
    key={event.id}
    title={event.title}
    date={event.date}
    category={event.category}
    categoryColor={event.categoryColor}
    image={event.image}
    region={event.region}
    source={event.source}
  />
));
```

### Vue Example

```javascript
// Fetch events
async fetchEvents() {
  const response = await fetch('https://your-api.com/combined-events');
  this.events = await response.json();
}

// Filter by category
computed: {
  culturalEvents() {
    return this.events.filter(e => e.category === 'Cultural');
  }
}
```

---

## üîß Testing

### 1. Test Transformer

```bash
python test_transformer.py
```

Creates sample events and transforms them.

### 2. Test API

```bash
python test_api.py
```

Tests all API endpoints.

### 3. Manual Scraper Test

```bash
python run_scrapers.py --headless --max-events 10
```

---

## üìà Monitoring in Production

### Check Health

```bash
curl https://your-api.com/health
```

### View Statistics

```bash
curl https://your-api.com/stats
```

### Check Scheduler

```bash
curl https://your-api.com/scheduler/status
```

### View Logs

**Railway:**
```bash
railway logs
```

**Docker:**
```bash
docker-compose logs -f api
```

---

## üí° Best Practices

### For Development
```bash
HEADLESS_MODE=False
SCRAPER_SCHEDULE=daily
SCRAPER_MAX_EVENTS=50
DATABASE_URL=sqlite:///./events_deals.db
```

### For Production
```bash
HEADLESS_MODE=True
SCRAPER_SCHEDULE=every_6_hours
SCRAPER_MAX_EVENTS=100
DATABASE_URL=postgresql://user:pass@host/db
SCRAPER_RUN_ON_STARTUP=True
```

---

## üéâ Next Steps

1. ‚úÖ **Test Locally**
   ```bash
   python setup.py
   python run_api.py
   ```

2. ‚úÖ **Test Transformation**
   ```bash
   python test_transformer.py
   ```

3. ‚úÖ **Deploy to Cloud**
   - Follow QUICKSTART.md for Railway
   - Or DEPLOYMENT.md for other platforms

4. ‚úÖ **Configure Scheduler**
   - Set `SCRAPER_SCHEDULE` in environment
   - Monitor with `/scheduler/status`

5. ‚úÖ **Integrate with Frontend**
   - Use `/combined-events` endpoint
   - Or query `/events` with filters

6. ‚úÖ **Monitor & Scale**
   - Check logs regularly
   - Adjust scraping frequency
   - Scale as needed

---

## üìö Documentation

- **[README.md](README.md)** - Main documentation
- **[QUICKSTART.md](QUICKSTART.md)** - 5-minute deploy
- **[DEPLOYMENT.md](DEPLOYMENT.md)** - Full deployment guide
- **[TRANSFORMER_GUIDE.md](TRANSFORMER_GUIDE.md)** - Data transformation
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - System architecture
- **[README_API.md](README_API.md)** - API documentation

---

## üÜò Support

### Common Issues

**Scrapers not running?**
- Check `/scheduler/status`
- Verify `SCRAPER_SCHEDULE` is set
- Check logs for errors

**No data in combined JSON?**
- Run scrapers first: `POST /scrape/sync`
- Check `scraped_data/combined_events.json`

**Database errors?**
- Check `DATABASE_URL` format
- Ensure database is accessible
- Check connection permissions

---

## ‚ú® Summary

You now have a **complete, production-ready system** that:

‚úÖ Scrapes 4 Greek event websites continuously
‚úÖ Transforms all data into standardized format
‚úÖ Saves to combined JSON file + database
‚úÖ Provides REST API for easy access
‚úÖ Runs on schedule automatically
‚úÖ Ready to deploy to cloud platforms
‚úÖ Fully documented and tested

**Your data is clean, consistent, and ready to use!**

---

Made with ‚ù§Ô∏è for the Greek events community
